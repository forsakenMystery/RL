{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import gym.spaces\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        super(NoopResetEnv, self).__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset()\n",
    "        noops = np.random.randint(1, self.noop_max + 1)\n",
    "        for _ in range(noops):\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def _reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, _, _ = self.env.step(1)\n",
    "        obs, _, _, _ = self.env.step(2)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        super(EpisodicLifeEnv, self).__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "        self.was_real_reset = False\n",
    "\n",
    "    def _step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset()\n",
    "            self.was_real_reset = True\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "            self.was_real_reset = False\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def _step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_frame84(frame):\n",
    "    img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "    img = Image.fromarray(img)\n",
    "    resized_screen = img.resize((84, 110), Image.BILINEAR)\n",
    "    resized_screen = np.array(resized_screen)\n",
    "    x_t = resized_screen[18:102, :]\n",
    "    x_t = np.reshape(x_t, [84, 84, 1])\n",
    "    return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
    "\n",
    "    def _step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return _process_frame84(obs), reward, done, info\n",
    "\n",
    "    def _reset(self):\n",
    "        return _process_frame84(self.env.reset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClippedRewardsWrapper(gym.Wrapper):\n",
    "    def _step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, np.sign(reward), done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_deepmind_ram(env):\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ClippedRewardsWrapper(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_deepmind(env):\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ClippedRewardsWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set Global seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import torch\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(task, seed):\n",
    "    env_id = task.env_id\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    set_global_seeds(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    expt_dir = 'tmp/gym-results'\n",
    "    env = wrappers.Monitor(env, expt_dir, force=True)\n",
    "    env = wrap_deepmind(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ram_env(env, seed):\n",
    "    set_global_seeds(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    expt_dir = '/tmp/gym-results'\n",
    "    env = wrappers.Monitor(env, expt_dir, force=True)\n",
    "    env = wrap_deepmind_ram(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrapper_by_name(env, classname):\n",
    "    currentenv = env\n",
    "    while True:\n",
    "        if classname in currentenv.__class__.__name__:\n",
    "            return currentenv\n",
    "        elif isinstance(env, gym.Wrapper):\n",
    "            currentenv = currentenv.env\n",
    "        else:\n",
    "            raise ValueError(\"Couldn't find wrapper named %s\"%classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_unique(sampling_f, n):\n",
    "    \"\"\"Helper function. Given a function `sampling_f` that returns\n",
    "    comparable objects, sample n such unique objects.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    while len(res) < n:\n",
    "        candidate = sampling_f()\n",
    "        if candidate not in res:\n",
    "            res.append(candidate)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size, frame_history_len):\n",
    "        \"\"\"This is a memory efficient implementation of the replay buffer.\n",
    "        The sepecific memory optimizations use here are:\n",
    "            - only store each frame once rather than k times\n",
    "              even if every observation normally consists of k last frames\n",
    "            - store frames as np.uint8 (actually it is most time-performance\n",
    "              to cast them back to float32 on GPU to minimize memory transfer\n",
    "              time)\n",
    "            - store frame_t and frame_(t+1) in the same buffer.\n",
    "        For the typical use case in Atari Deep RL buffer with 1M frames the total\n",
    "        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n",
    "        Warning! Assumes that returning frame of zeros at the beginning\n",
    "        of the episode, when there is less frames than `frame_history_len`,\n",
    "        is acceptable.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        frame_history_len: int\n",
    "            Number of memories to be retried for each observation.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_history_len = frame_history_len\n",
    "\n",
    "        self.next_idx      = 0\n",
    "        self.num_in_buffer = 0\n",
    "\n",
    "        self.obs      = None\n",
    "        self.action   = None\n",
    "        self.reward   = None\n",
    "        self.done     = None\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
    "        return batch_size + 1 <= self.num_in_buffer\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obs_batch      = np.concatenate([self._encode_observation(idx)[np.newaxis, :] for idx in idxes], 0)\n",
    "        act_batch      = self.action[idxes]\n",
    "        rew_batch      = self.reward[idxes]\n",
    "        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[np.newaxis, :] for idx in idxes], 0)\n",
    "        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
    "\n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample `batch_size` different transitions.\n",
    "        i-th sample transition is the following:\n",
    "        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n",
    "        after which reward `rew_batch[i]` was received and subsequent\n",
    "        observation  next_obs_batch[i] was observed, unless the epsiode\n",
    "        was done which is represented by `done_mask[i]` which is equal\n",
    "        to 1 if episode has ended as a result of that action.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_c * frame_history_len, img_h, img_w)\n",
    "            and dtype np.uint8\n",
    "        act_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "        rew_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        next_obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_c * frame_history_len, img_h, img_w)\n",
    "            and dtype np.uint8\n",
    "        done_mask: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        \"\"\"\n",
    "        assert self.can_sample(batch_size)\n",
    "        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "    def encode_recent_observation(self):\n",
    "        \"\"\"Return the most recent `frame_history_len` frames.\n",
    "        Returns\n",
    "        -------\n",
    "        observation: np.array\n",
    "            Array of shape (img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n",
    "            encodes frame at time `t - frame_history_len + i`\n",
    "        \"\"\"\n",
    "        assert self.num_in_buffer > 0\n",
    "        return self._encode_observation((self.next_idx - 1) % self.size)\n",
    "\n",
    "    def _encode_observation(self, idx):\n",
    "        end_idx   = idx + 1 # make noninclusive\n",
    "        start_idx = end_idx - self.frame_history_len\n",
    "        # this checks if we are using low-dimensional observations, such as RAM\n",
    "        # state, in which case we just directly return the latest RAM.\n",
    "        if len(self.obs.shape) == 2:\n",
    "            return self.obs[end_idx-1]\n",
    "        # if there weren't enough frames ever in the buffer for context\n",
    "        if start_idx < 0 and self.num_in_buffer != self.size:\n",
    "            start_idx = 0\n",
    "        for idx in range(start_idx, end_idx - 1):\n",
    "            if self.done[idx % self.size]:\n",
    "                start_idx = idx + 1\n",
    "        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "        # if zero padding is needed for missing context\n",
    "        # or we are on the boundry of the buffer\n",
    "        if start_idx < 0 or missing_context > 0:\n",
    "            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                frames.append(self.obs[idx % self.size])\n",
    "            return np.concatenate(frames, 0)\n",
    "        else:\n",
    "            # this optimization has potential to saves about 30% compute time \\o/\n",
    "            img_h, img_w = self.obs.shape[2], self.obs.shape[3]\n",
    "            return self.obs[start_idx:end_idx].reshape(-1, img_h, img_w)\n",
    "\n",
    "    def store_frame(self, frame):\n",
    "        \"\"\"Store a single frame in the buffer at the next available index, overwriting\n",
    "        old frames if necessary.\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: np.array\n",
    "            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
    "            and the frame will transpose to shape (img_h, img_w, img_c) to be stored\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            Index at which the frame is stored. To be used for `store_effect` later.\n",
    "        \"\"\"\n",
    "        # make sure we are not using low-dimensional observations, such as RAM\n",
    "        if len(frame.shape) > 1:\n",
    "            # transpose image frame into (img_c, img_h, img_w)\n",
    "            frame = frame.transpose(2, 0, 1)\n",
    "\n",
    "        if self.obs is None:\n",
    "            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n",
    "            self.action   = np.empty([self.size],                     dtype=np.int32)\n",
    "            self.reward   = np.empty([self.size],                     dtype=np.float32)\n",
    "            self.done     = np.empty([self.size],                     dtype=np.bool)\n",
    "\n",
    "        self.obs[self.next_idx] = frame\n",
    "\n",
    "        ret = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1) % self.size\n",
    "        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def store_effect(self, idx, action, reward, done):\n",
    "        \"\"\"Store effects of action taken after obeserving frame stored\n",
    "        at index idx. The reason `store_frame` and `store_effect` is broken\n",
    "        up into two functions is so that one can call `encode_recent_observation`\n",
    "        in between.\n",
    "        Paramters\n",
    "        ---------\n",
    "        idx: int\n",
    "            Index in buffer of recently observed frame (returned by `store_frame`).\n",
    "        action: int\n",
    "            Action that was performed upon observing this frame.\n",
    "        reward: float\n",
    "            Reward that was received when the actions was performed.\n",
    "        done: bool\n",
    "            True if episode was finished after performing that action.\n",
    "        \"\"\"\n",
    "        self.action[idx] = action\n",
    "        self.reward[idx] = reward\n",
    "        self.done[idx]   = done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedualing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schedule(object):\n",
    "    def value(self, t):\n",
    "        \"\"\"Value of the schedule at time t\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantSchedule(object):\n",
    "    def __init__(self, value):\n",
    "        \"\"\"Value remains constant over time.\n",
    "        Parameters\n",
    "        ----------\n",
    "        value: float\n",
    "            Constant value of the schedule\n",
    "        \"\"\"\n",
    "        self._v = value\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        return self._v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(l, r, alpha):\n",
    "    return l + alpha * (r - l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseSchedule(object):\n",
    "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
    "        \"\"\"Piecewise schedule.\n",
    "        endpoints: [(int, int)]\n",
    "            list of pairs `(time, value)` meanining that schedule should output\n",
    "            `value` when `t==time`. All the values for time must be sorted in\n",
    "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
    "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
    "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
    "            time passed between `time_a` and `time_b` for time `t`.\n",
    "        interpolation: lambda float, float, float: float\n",
    "            a function that takes value to the left and to the right of t according\n",
    "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
    "            right endpoint that t has covered. See linear_interpolation for example.\n",
    "        outside_value: float\n",
    "            if the value is requested outside of all the intervals sepecified in\n",
    "            `endpoints` this value is returned. If None then AssertionError is\n",
    "            raised when outside value is requested.\n",
    "        \"\"\"\n",
    "        idxes = [e[0] for e in endpoints]\n",
    "        assert idxes == sorted(idxes)\n",
    "        self._interpolation = interpolation\n",
    "        self._outside_value = outside_value\n",
    "        self._endpoints      = endpoints\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
    "            if l_t <= t and t < r_t:\n",
    "                alpha = float(t - l_t) / (r_t - l_t)\n",
    "                return self._interpolation(l, r, alpha)\n",
    "\n",
    "        # t does not belong to any of the pieces, so doom.\n",
    "        assert self._outside_value is not None\n",
    "        return self._outside_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        \"\"\"Linear interpolation between initial_p and final_p over\n",
    "        schedule_timesteps. After this many timesteps pass final_p is\n",
    "        returned.\n",
    "        Parameters\n",
    "        ----------\n",
    "        schedule_timesteps: int\n",
    "            Number of timesteps for which to linearly anneal initial_p\n",
    "            to final_p\n",
    "        initial_p: float\n",
    "            initial output value\n",
    "        final_p: float\n",
    "            final output value\n",
    "        \"\"\"\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p            = final_p\n",
    "        self.initial_p          = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        fraction  = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=18):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_RAM(nn.Module):\n",
    "    def __init__(self, in_features=4, num_actions=18):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network for testing algorithm\n",
    "            in_features: number of features of input.\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN_RAM, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(autograd.Variable):\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        if USE_CUDA:\n",
    "            data = data.cuda()\n",
    "        super(Variable, self).__init__(data, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Statistic = {\n",
    "    \"mean_episode_rewards\": [],\n",
    "    \"best_mean_episode_rewards\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_learing(\n",
    "    env,\n",
    "    q_func,\n",
    "    optimizer_spec,\n",
    "    exploration,\n",
    "    stopping_criterion=None,\n",
    "    replay_buffer_size=1000000,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    learning_starts=50000,\n",
    "    learning_freq=4,\n",
    "    frame_history_len=4,\n",
    "    target_update_freq=10000\n",
    "    ):\n",
    "\n",
    "    \"\"\"Run Deep Q-learning algorithm.\n",
    "    You can specify your own convnet using q_func.\n",
    "    All schedules are w.r.t. total number of steps taken in the environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.Env\n",
    "        gym environment to train on.\n",
    "    q_func: function\n",
    "        Model to use for computing the q function. It should accept the\n",
    "        following named arguments:\n",
    "            input_channel: int\n",
    "                number of channel of input.\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "    optimizer_spec: OptimizerSpec\n",
    "        Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "        for the optimizer\n",
    "    exploration: Schedule (defined in utils.schedule)\n",
    "        schedule for probability of chosing random action.\n",
    "    stopping_criterion: (env) -> bool\n",
    "        should return true when it's ok for the RL algorithm to stop.\n",
    "        takes in env and the number of steps executed so far.\n",
    "    replay_buffer_size: int\n",
    "        How many memories to store in the replay buffer.\n",
    "    batch_size: int\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma: float\n",
    "        Discount Factor\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    frame_history_len: int\n",
    "        How many past frames to include as input to the model.\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.Box\n",
    "    assert type(env.action_space)      == gym.spaces.Discrete\n",
    "\n",
    "    ###############\n",
    "    # BUILD MODEL #\n",
    "    ###############\n",
    "\n",
    "    if len(env.observation_space.shape) == 1:\n",
    "        # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "        input_arg = env.observation_space.shape[0]\n",
    "    else:\n",
    "        img_h, img_w, img_c = env.observation_space.shape\n",
    "        input_arg = frame_history_len * img_c\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Construct an epilson greedy policy with given exploration schedule\n",
    "    def select_epilson_greedy_action(model, obs, t):\n",
    "        sample = random.random()\n",
    "        eps_threshold = exploration.value(t)\n",
    "        if sample > eps_threshold:\n",
    "            obs = torch.from_numpy(obs).type(dtype).unsqueeze(0) / 255.0\n",
    "            # Use volatile = True if variable is only used in inference mode, i.e. donâ€™t save the history\n",
    "            return model(Variable(obs, volatile=True)).data.max(1)[1].cpu()\n",
    "        else:\n",
    "            return torch.IntTensor([[random.randrange(num_actions)]])\n",
    "\n",
    "    # Initialize target q function and q function\n",
    "    Q = q_func(input_arg, num_actions).type(dtype)\n",
    "    target_Q = q_func(input_arg, num_actions).type(dtype)\n",
    "\n",
    "    # Construct Q network optimizer function\n",
    "    optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n",
    "\n",
    "    # Construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "    ###############\n",
    "    # RUN ENV     #\n",
    "    ###############\n",
    "    num_param_updates = 0\n",
    "    mean_episode_reward = -float('nan')\n",
    "    best_mean_episode_reward = -float('inf')\n",
    "    last_obs = env.reset()\n",
    "    LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "    for t in count():\n",
    "        ### Check stopping criterion\n",
    "        if stopping_criterion is not None and stopping_criterion(env):\n",
    "            break\n",
    "\n",
    "        ### Step the env and store the transition\n",
    "        # Store lastest observation in replay memory and last_idx can be used to store action, reward, done\n",
    "        last_idx = replay_buffer.store_frame(last_obs)\n",
    "        # encode_recent_observation will take the latest observation\n",
    "        # that you pushed into the buffer and compute the corresponding\n",
    "        # input that should be given to a Q network by appending some\n",
    "        # previous frames.\n",
    "        recent_observations = replay_buffer.encode_recent_observation()\n",
    "\n",
    "        # Choose random action if not yet start learning\n",
    "        if t > learning_starts:\n",
    "            action = select_epilson_greedy_action(Q, recent_observations, t)[0, 0]\n",
    "        else:\n",
    "            action = random.randrange(num_actions)\n",
    "        # Advance one step\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        # clip rewards between -1 and 1\n",
    "        reward = max(-1.0, min(reward, 1.0))\n",
    "        # Store other info in replay memory\n",
    "        replay_buffer.store_effect(last_idx, action, reward, done)\n",
    "        # Resets the environment when reaching an episode boundary.\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        last_obs = obs\n",
    "\n",
    "        ### Perform experience replay and train the network.\n",
    "        # Note that this is only done if the replay buffer contains enough samples\n",
    "        # for us to learn something useful -- until then, the model will not be\n",
    "        # initialized and random actions should be taken\n",
    "        if (t > learning_starts and\n",
    "                t % learning_freq == 0 and\n",
    "                replay_buffer.can_sample(batch_size)):\n",
    "            # Use the replay buffer to sample a batch of transitions\n",
    "            # Note: done_mask[i] is 1 if the next state corresponds to the end of an episode,\n",
    "            # in which case there is no Q-value at the next state; at the end of an\n",
    "            # episode, only the current state reward contributes to the target\n",
    "            obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
    "            # Convert numpy nd_array to torch variables for calculation\n",
    "            obs_batch = Variable(torch.from_numpy(obs_batch).type(dtype) / 255.0)\n",
    "            act_batch = Variable(torch.from_numpy(act_batch).long())\n",
    "            rew_batch = Variable(torch.from_numpy(rew_batch))\n",
    "            next_obs_batch = Variable(torch.from_numpy(next_obs_batch).type(dtype) / 255.0)\n",
    "            not_done_mask = Variable(torch.from_numpy(1 - done_mask)).type(dtype)\n",
    "\n",
    "            if USE_CUDA:\n",
    "                act_batch = act_batch.cuda()\n",
    "                rew_batch = rew_batch.cuda()\n",
    "\n",
    "            # Compute current Q value, q_func takes only state and output value for every state-action pair\n",
    "            # We choose Q based on action taken.\n",
    "            current_Q_values = Q(obs_batch).gather(1, act_batch.unsqueeze(1))\n",
    "            # Compute next Q value based on which action gives max Q values\n",
    "            # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "            next_max_q = target_Q(next_obs_batch).detach().max(1)[0]\n",
    "            next_Q_values = not_done_mask * next_max_q\n",
    "            # Compute the target of the current Q values\n",
    "            target_Q_values = rew_batch + (gamma * next_Q_values)\n",
    "            # Compute Bellman error\n",
    "            bellman_error = target_Q_values - current_Q_values\n",
    "            # clip the bellman error between [-1 , 1]\n",
    "            clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
    "            # Note: clipped_bellman_delta * -1 will be right gradient\n",
    "            d_error = clipped_bellman_error * -1.0\n",
    "            # Clear previous gradients before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # run backward pass\n",
    "            current_Q_values.backward(d_error.data.unsqueeze(1))\n",
    "\n",
    "            # Perfom the update\n",
    "            optimizer.step()\n",
    "            num_param_updates += 1\n",
    "\n",
    "            # Periodically update the target network by Q network to target Q network\n",
    "            if num_param_updates % target_update_freq == 0:\n",
    "                target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "        ### 4. Log progress and keep track of statistics\n",
    "        episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "        if len(episode_rewards) > 0:\n",
    "            mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "        if len(episode_rewards) > 100:\n",
    "            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "\n",
    "        Statistic[\"mean_episode_rewards\"].append(mean_episode_reward)\n",
    "        Statistic[\"best_mean_episode_rewards\"].append(best_mean_episode_reward)\n",
    "\n",
    "        if t % LOG_EVERY_N_STEPS == 0 and t > learning_starts:\n",
    "            print(\"Timestep %d\" % (t,))\n",
    "            print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "            print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "            print(\"episodes %d\" % len(episode_rewards))\n",
    "            print(\"exploration %f\" % exploration.value(t))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Dump statistics to pickle\n",
    "            with open('statistics.pkl', 'wb') as f:\n",
    "                pickle.dump(Statistic, f)\n",
    "                print(\"Saved to %s\" % 'statistics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "LEARNING_STARTS = 50000\n",
    "LEARNING_FREQ = 4\n",
    "FRAME_HISTORY_LEN = 4\n",
    "TARGER_UPDATE_FREQ = 10000\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env, num_timesteps=int(4e7)):\n",
    "\n",
    "    def stopping_criterion(env):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "\n",
    "    optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    "    )\n",
    "\n",
    "    exploration_schedule = LinearSchedule(1000000, 0.1)\n",
    "\n",
    "    dqn_learing(\n",
    "        env=env,\n",
    "        q_func=DQN_RAM,\n",
    "        optimizer_spec=optimizer_spec,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        learning_freq=LEARNING_FREQ,\n",
    "        frame_history_len=FRAME_HISTORY_LEN,\n",
    "        target_update_freq=TARGER_UPDATE_FREQ,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192,   0,   0,   0, 110,  38,   0,   7,  71,   1,  60,  59,   0,\n",
       "         0,   0,  62, 255,   0, 255, 253,   0,  22,   0,  24, 128,  32,\n",
       "         1,  86, 247,  86, 247,  86, 247, 134, 243, 245, 243, 240, 240,\n",
       "       242, 242,  32,  32,  64,  64,  64, 188,  65, 189,   0,  22, 109,\n",
       "        37,  37,  60,   0,   0,   0,   0, 109, 109,  37,  37, 192, 192,\n",
       "       192, 192,   1, 192, 202, 247, 202, 247, 202, 247, 202, 247,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  54, 236, 242, 121, 240], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-f3ba7604ac57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ram_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-777671284a9b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(env, num_timesteps)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mlearning_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNING_FREQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mframe_history_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFRAME_HISTORY_LEN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtarget_update_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTARGER_UPDATE_FREQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     )\n",
      "\u001b[1;32m<ipython-input-29-6a22a4795335>\u001b[0m in \u001b[0;36mdqn_learing\u001b[1;34m(env, q_func, optimizer_spec, exploration, stopping_criterion, replay_buffer_size, batch_size, gamma, learning_starts, learning_freq, frame_history_len, target_update_freq)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mmean_episode_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nan'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mbest_mean_episode_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mlast_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[0mLOG_EVERY_N_STEPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\gitting\\gym\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = get_ram_env(env, seed)\n",
    "\n",
    "main(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
